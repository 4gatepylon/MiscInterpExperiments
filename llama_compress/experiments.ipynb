{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Tuple\n",
    "\n",
    "# def compression_experiment(\n",
    "#     model_name: str = \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "#     text: str = \"\",\n",
    "#     seed_length: int = 10,\n",
    "#     max_new_tokens: int = 1000\n",
    "# ):\n",
    "#     \"\"\"See if we can use a language model to try and compress text reasonably well.\"\"\"\n",
    "#     # Load model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "seed_length = 40\n",
    "print(f\"Loading model and tokenizer: {model_name}\")\n",
    "device = \"cuda:0\"\n",
    "# Generate text based on the seed\n",
    "# print(\"Generating text from seed...\")\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         seed_tokens,\n",
    "#         max_new_tokens=min(max_new_tokens, full_encoding.shape[1] - seed_length),\n",
    "#         do_sample=False  # Use greedy decoding for deterministic output\n",
    "#     )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with open(\"gutenberg_book.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "# compression_experiment(\n",
    "#     text=example_text,\n",
    "#     seed_length=20,\n",
    "#     max_new_tokens=200\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\").to(device)\n",
    "\n",
    "# Tokenize the entire text\n",
    "full_encoding = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Extract the seed (first L tokens)\n",
    "if seed_length >= full_encoding.shape[1]:\n",
    "    raise ValueError(f\"Seed length ({seed_length}) must be less than the total number of tokens ({full_encoding.shape[1]})\")\n",
    "\n",
    "max_new_tokens = 999\n",
    "seed_tokens = full_encoding[:, :seed_length]\n",
    "target_tokens = full_encoding[:, seed_length:min(seed_length + max_new_tokens, full_encoding.shape[1])]\n",
    "print(\"seed tokens: \", seed_tokens.shape, seed_tokens.device)\n",
    "print(\"target tokens: \", target_tokens.shape, target_tokens.device)\n",
    "print(\"model.device: \", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import einops\n",
    "import io\n",
    "import contextlib\n",
    "from typing import Dict\n",
    "devices = {pn: str(sx.device) for pn, sx in model.named_parameters()}\n",
    "print(json.dumps(devices, indent=4))\n",
    "assert set(devices.values()) == {\"cuda:0\"}\n",
    "assert seed_tokens.ndim == target_tokens.ndim <= 2, f\"seed_tokens.ndim: {seed_tokens.ndim}, target_tokens.ndim: {target_tokens.ndim}\" # fmt: skip\n",
    "assert (seed_tokens.ndim == 1) or (seed_tokens.shape[0] == target_tokens.shape[0] == 1), f\"seed_tokens.shape[0]: {seed_tokens.shape[0]}, target_tokens.shape[0]: {target_tokens.shape[0]}\" # fmt: skip\n",
    "seed_tokens = seed_tokens.flatten()\n",
    "target_tokens = target_tokens.flatten()\n",
    "running_amt = len(seed_tokens)\n",
    "zero_pad_start = torch.zeros_like(seed_tokens)\n",
    "zero_pad_end = torch.zeros_like(target_tokens)\n",
    "#....\n",
    "seed_tokens_padded = einops.rearrange(torch.cat([seed_tokens, zero_pad_end]), \"b -> 1 b\") # fmt: skip\n",
    "target_tokens_padded = einops.rearrange(torch.cat([zero_pad_start, target_tokens]), \"b -> 1 b\") # fmt: skip\n",
    "print(seed_tokens_padded.shape, target_tokens_padded.shape, \"from\", seed_tokens.shape, target_tokens.shape) # fmt: skip\n",
    "assert seed_tokens_padded.device == torch.device(\"cuda:0\")\n",
    "assert target_tokens_padded.device == torch.device(\"cuda:0\")\n",
    "assert seed_tokens_padded.shape == target_tokens_padded.shape\n",
    "# model(seed_tokens).logits.shape # batch seq vocab\n",
    "correction_on_running_amt: Dict[int, torch.Tensor] = {}\n",
    "assert target_tokens_padded.shape[0] == 1, f\"target_tokens_padded.shape[0]: {target_tokens_padded.shape[0]}\" # fmt: skip\n",
    "assert running_amt < target_tokens_padded.shape[1], f\"running_amt: {running_amt}, target_tokens_padded.shape[1]: {target_tokens_padded.shape[1]}\" # fmt: skip\n",
    "for i in tqdm.trange(running_amt, target_tokens_padded.shape[1], desc=f\"Running inference... run from [{running_amt}, {target_tokens_padded.shape[1]})\"): # fmt: skip\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        prediction = model(seed_tokens_padded[:, :i]).logits[:, -1, :].argmax(dim=-1)\n",
    "        if (prediction == target_tokens_padded[0, i]).all():\n",
    "            seed_tokens_padded[0, i] = prediction\n",
    "        else:\n",
    "            correction_on_running_amt[i] = target_tokens_padded[0, i]\n",
    "            seed_tokens_padded[0, i] = target_tokens_padded[0, i]\n",
    "        assert (seed_tokens_padded[0, i] == target_tokens_padded[0, i]).all()\n",
    "print(\"Total number wrong: \", len(correction_on_running_amt))\n",
    "print(\"Hypthetical compression ratio: \", 1 / ((len(seed_tokens.flatten()) + 2 * len(correction_on_running_amt)) / len(seed_tokens_padded.flatten()))) # ignoring model since we assume that will go to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Try one other dataset\"\"\"\n",
    "from datasets import load_dataset\n",
    "fb_natural_reasoning_hf = load_dataset(\"facebook/natural_reasoning\", split=\"train\").select(range(1000)) # fmt: skip\n",
    "questions = [x[\"question\"] for x in fb_natural_reasoning_hf]\n",
    "questions_mega_str = \"\\n\\n\".join(questions)\n",
    "questions_tok = tokenizer.encode(questions_mega_str, return_tensors=\"pt\").to(model.device)\n",
    "questions_tok = questions_tok[:, :min(max_new_tokens, questions_tok.shape[1])]\n",
    "assert isinstance(questions_tok, torch.Tensor), f\"questions_tok: {type(questions_tok)}\"\n",
    "assert questions_tok.ndim == 2, f\"questions_tok.ndim: {questions_tok.ndim}\"\n",
    "assert questions_tok.shape[0] == 1, f\"questions_tok.shape[0]: {questions_tok.shape[0]}\"\n",
    "questions_tok_seed = torch.zeros_like(questions_tok)\n",
    "questions_tok_tgt = torch.zeros_like(questions_tok)\n",
    "# Now we will jsut use the strategy from above...\n",
    "questions_tok_seed[:, :seed_length] = questions_tok[:, :seed_length]\n",
    "questions_tok_tgt[:, seed_length:] = questions_tok[:, seed_length:]\n",
    "print(\"questions_tok.shape: \", questions_tok.shape)\n",
    "assert questions_tok.ndim == 2, f\"questions_tok.ndim: {questions_tok.ndim}\"\n",
    "assert questions_tok.shape[0] == 1, f\"questions_tok.shape[0]: {questions_tok.shape[0]}\"\n",
    "correction_on_running_amt: Dict[int, torch.Tensor] = {}\n",
    "for i in tqdm.trange(seed_length, questions_tok_tgt.shape[1], desc=f\"Running inference... run from [{seed_length}, {questions_tok_tgt.shape[1]})\"): # fmt: skip\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        prediction = model(questions_tok_seed[:, :i]).logits[:, -1, :].argmax(dim=-1)\n",
    "        if (prediction == questions_tok_tgt[0, i]).all():\n",
    "            questions_tok_seed[0, i] = prediction\n",
    "        else:\n",
    "            correction_on_running_amt[i] = questions_tok_tgt[0, i]\n",
    "            questions_tok_seed[0, i] = questions_tok_tgt[0, i]\n",
    "        assert (questions_tok_seed[0, i] == questions_tok_tgt[0, i]).all()\n",
    "print(\"Total number wrong: \", len(correction_on_running_amt))\n",
    "print(\"Hypthetical compression ratio: \", 1 / ((len(questions_tok_seed.flatten()) + 2 * len(correction_on_running_amt)) / len(questions_tok_seed.flatten()))) # ignoring model since we assume that will go to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try recursive compression LOL\n",
    "\n",
    "Language modeling is compression: https://arxiv.org/abs/2309.10668\n",
    "\n",
    "It sems like 10:1 might be possible with `https://bellard.org/nncp/` for language on\n",
    "enwiki8\n",
    "\n",
    "conclusion is this is not super good ngl (probably need a better algorithm/model if this\n",
    "is to work at all)\n",
    "\"\"\"\n",
    "# wrong_str = \"\".join([f\"{k}{v}\" for k, v in correction_on_running_amt.items()]) # <---- can you predict all the data in a dumb way?\n",
    "wrong_str = \",\".join([f\"{k}\" for k, v in correction_on_running_amt.items()]) # <----- can you predict just the indices?\n",
    "wrong_str_tok = tokenizer.encode(wrong_str, return_tensors=\"pt\").to(model.device)\n",
    "print(wrong_str_tok.shape)\n",
    "wrong_str_seed = torch.zeros_like(wrong_str_tok)\n",
    "wrong_str_seed[:, :seed_length] = wrong_str_tok[:, :seed_length]\n",
    "wrong_str_tgt = torch.zeros_like(wrong_str_tok)\n",
    "wrong_str_tgt[:, seed_length:] = wrong_str_tok[:, seed_length:]\n",
    "n_wrong = 0\n",
    "for i in tqdm.trange(seed_length, wrong_str_tgt.shape[1], desc=f\"Running inference... run from [{seed_length}, {wrong_str_tgt.shape[1]})\"): # fmt: skip\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        prediction = model(wrong_str_seed[:, :i]).logits[:, -1, :].argmax(dim=-1)\n",
    "        if (prediction == wrong_str_tgt[0, i]).all():\n",
    "            wrong_str_seed[0, i] = prediction\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "print(f\"Total number wrong: {n_wrong}\")\n",
    "print(f\"Percent wrong: n_wrong/total = {n_wrong / wrong_str_tgt.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
