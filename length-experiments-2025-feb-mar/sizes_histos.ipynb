{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num jsons:  90\n",
      "{('awesome', 2): 6, ('gsm8k', 2): 6, ('leetcode', 2): 6, ('reasoning', 2): 6, ('awesome', 3): 6, ('gsm8k', 3): 6, ('leetcode', 3): 6, ('reasoning', 3): 6, ('awesome', 4): 6, ('gsm8k', 4): 6, ('leetcode', 4): 6, ('reasoning', 4): 6, ('awesome', 5): 6, ('gsm8k', 5): 6, ('leetcode', 5): 6, ('reasoning', 5): 6, ('awesome', 6): 6, ('gsm8k', 6): 6, ('leetcode', 6): 6, ('reasoning', 6): 6, ('awesome', 7): 6, ('gsm8k', 7): 6, ('leetcode', 7): 6, ('reasoning', 7): 6, ('awesome', 8): 6, ('gsm8k', 8): 6, ('leetcode', 8): 6, ('reasoning', 8): 6, ('awesome', 9): 6, ('gsm8k', 9): 6, ('leetcode', 9): 6, ('reasoning', 9): 6, ('awesome', 10): 6, ('gsm8k', 10): 6, ('leetcode', 10): 6, ('reasoning', 10): 6, ('awesome', 11): 6, ('gsm8k', 11): 6, ('leetcode', 11): 6, ('reasoning', 11): 6, ('awesome', 12): 6, ('gsm8k', 12): 6, ('leetcode', 12): 6, ('reasoning', 12): 6, ('awesome', 13): 6, ('gsm8k', 13): 6, ('leetcode', 13): 6, ('reasoning', 13): 6, ('awesome', 14): 6, ('gsm8k', 14): 6, ('leetcode', 14): 6, ('reasoning', 14): 6, ('awesome', 15): 6, ('gsm8k', 15): 6, ('leetcode', 15): 6, ('reasoning', 15): 6, ('awesome', 16): 6, ('gsm8k', 16): 6, ('leetcode', 16): 6, ('reasoning', 16): 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets: 100%|██████████| 60/60 [00:15<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import json\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\"\"\"\n",
    "8B04545E-B159-4A65-AE77-D474D853FE2E\n",
    "\n",
    "We simply look at the length statistics of both the generations:\n",
    "\"\"\"\n",
    "assert os.environ.get(\"USEABLES_DIR\", None) is not None\n",
    "results_dir = Path.cwd() / os.environ[\"USEABLES_DIR\"] / \"steering_generations\"\n",
    "# assert os.environ.get(\"OUTPUT_DIR\", None) is not None\n",
    "# results_dir = Path.cwd() / os.environ[\"OUTPUT_DIR\"] / \"batched_steering\"\n",
    "assert results_dir.exists() and results_dir.is_dir()\n",
    "assert all(x.is_file() for x in results_dir.iterdir())\n",
    "jsons = list(results_dir.glob(\"*.json\"))\n",
    "dataset_layer_all_histos = {}\n",
    "class AllHistosInfo(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    mean: float\n",
    "    max: float\n",
    "    min: float\n",
    "    stdev: float\n",
    "    stderr: float\n",
    "    lens: np.ndarray\n",
    "    mag: float\n",
    "    layer: int\n",
    "\n",
    "def get_mag_layer(name: str) -> tuple[float, int]:\n",
    "    mag, layer = float(name.split(\"mag\")[1].split(\"_\")[0]), int(name.split(\"_layer\")[1].split(\".\")[0])\n",
    "    return mag, layer\n",
    "print(\"Total num jsons: \", len(jsons))\n",
    "for j in jsons:\n",
    "    # mag1.0_layer8.json\n",
    "    # Extract magnitude and layer from the filename\n",
    "    mag, layer = get_mag_layer(j.name)\n",
    "    # print(f\"mag={mag}, layer={layer}\") # DEBUG\n",
    "    dataset2lens = {d: np.array([len(x) for x in xs]) for d, xs in json.loads(j.read_text()).items()} # dict of dataset name: list of interactions\n",
    "    dataset2means= {d: x.mean() for d, x in dataset2lens.items()}\n",
    "    dataset2min= {d: x.min() for d, x in dataset2lens.items()}\n",
    "    dataset2max= {d: x.max() for d, x in dataset2lens.items()}\n",
    "    dataset2std= {d: x.std() for d, x in dataset2lens.items()}\n",
    "    dataset2stderr= {d: x.std() / len(x) for d, x in dataset2lens.items()}\n",
    "    # for dataset in dataset2lens:\n",
    "    #     print(f\"@mag={mag}, layer={layer}, dataset={dataset}, mean={dataset2means[dataset]:.2f}, min={dataset2min[dataset]}, max={dataset2max[dataset]}, std={dataset2std[dataset]:.2f}, stderr={dataset2stderr[dataset]:.4f}\") # fmt: skip\n",
    "    # assert len(dataset_layer_all_histos.keys()) == 0 or set(dataset2lens.keys()) == set(dataset_layer_all_histos.keys())\n",
    "    for dataset in dataset2lens:\n",
    "        key = (dataset, layer)\n",
    "        if key not in dataset_layer_all_histos:\n",
    "            dataset_layer_all_histos[key] = []\n",
    "        dataset_layer_all_histos[key].append(AllHistosInfo(\n",
    "            mean=dataset2means[dataset].item(),\n",
    "            max=dataset2max[dataset].item(),\n",
    "            min=dataset2min[dataset].item(),\n",
    "            stdev=dataset2std[dataset].item(),\n",
    "            stderr=dataset2stderr[dataset].item(),\n",
    "            lens=dataset2lens[dataset],\n",
    "            mag=mag,\n",
    "            layer=layer,\n",
    "        ))\n",
    "\n",
    "# Get a list of distinct colors from matplotlib's color maps\n",
    "colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "assert len(colors) == len(set(colors))\n",
    "dataset_layer_all_histos = {\n",
    "    k: sorted(v, key=lambda x: x.mag) for k, v in dataset_layer_all_histos.items()\n",
    "}\n",
    "items = dataset_layer_all_histos.items()\n",
    "items = sorted(items, key=lambda x: x[0][0]) # Sort first by dataset (inner ordering)\n",
    "items = sorted(items, key=lambda x: x[0][1]) # Sort second by layer (outer ordering)\n",
    "num_histos_per_layer_dataset = {k: len(v) for k, v in items}\n",
    "print(num_histos_per_layer_dataset)\n",
    "num_histos_per_layer_dataset = np.array([len(v) for v in dataset_layer_all_histos.values()])\n",
    "_max, _min = num_histos_per_layer_dataset.max(), num_histos_per_layer_dataset.min()\n",
    "assert _max == _min\n",
    "output_dir = Path.cwd() / os.environ[\"OUTPUT_DIR\"] / \"viz\"\n",
    "if output_dir.exists() and len(list(output_dir.iterdir())) > 0:\n",
    "    shutil.rmtree(output_dir)\n",
    "assert not output_dir.exists()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "do_show: bool = False\n",
    "for i, ((dataset, layer), all_histos) in enumerate(tqdm.tqdm(items, desc=\"Processing datasets\")):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    assert len(all_histos) <= len(colors)\n",
    "    for j, histo in enumerate(all_histos):\n",
    "        color = colors[j % len(colors)]\n",
    "        plt.hist(histo.lens, bins=20, alpha=0.5, color=color, label=f'mag={histo.mag}, (mean={histo.mean:.2f}, stderr={histo.stderr:.4f})')\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        ymin, ymax = plt.ylim()\n",
    "        plt.vlines(x=histo.mean, ymin=0, ymax=ymax*0.9, color=color, linestyle='--', linewidth=2)\n",
    "        \n",
    "        # Add horizontal error bar for stderr\n",
    "        plt.hlines(y=ymax*0.85-j*(ymax*0.05), xmin=histo.mean-histo.stderr, xmax=histo.mean+histo.stderr, \n",
    "                  color='black', linewidth=2)\n",
    "        plt.plot([histo.mean-histo.stderr, histo.mean+histo.stderr], [ymax*0.85-j*(ymax*0.05), ymax*0.85-j*(ymax*0.05)], \n",
    "                 color='black', marker='|', markersize=8, linestyle='')\n",
    "    \n",
    "    plt.title(f\"Layer `{layer}`, dataset `{dataset}` Length Distribution (Total samples: {len(all_histos)})\")\n",
    "    plt.xlabel(\"Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    if do_show:\n",
    "        plt.show()\n",
    "    plt.savefig(output_dir / f\"length_histogram_{dataset}_{layer}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables visualization saved to /mnt/align4_drive2/adrianoh/git2/MiscInterpExperiments/length-experiments-2025-feb-mar/output_steering_experiments/viz/magnitude_impact_tables.png\n"
     ]
    }
   ],
   "source": [
    "# Create visualization of tables showing percent of zero-magnitude mean length\n",
    "\n",
    "# Helper functions\n",
    "def extract_data_tensor(items: list[tuple[tuple[str, int], list[AllHistosInfo]]]) -> tuple[np.ndarray, list[str], list[int], list[float]]: # fmt: skip\n",
    "    \"\"\"Extract data into a tensor of shape (n_datasets, n_layers, n_mags)\"\"\"\n",
    "    # Get unique datasets, layers, and magnitudes\n",
    "    datasets = sorted(set(item[0][0] for item in items))\n",
    "    layers = sorted(set(item[0][1] for item in items))\n",
    "    all_mags = sorted(set(histo.mag for _, histos in items for histo in histos))\n",
    "    \n",
    "    # Create mapping dictionaries for indexing\n",
    "    dataset_to_idx = {dataset: i for i, dataset in enumerate(datasets)}\n",
    "    layer_to_idx = {layer: i for i, layer in enumerate(layers)}\n",
    "    mag_to_idx = {mag: i for i, mag in enumerate(all_mags)}\n",
    "    assert len(items) == len(datasets) * len(layers)\n",
    "    assert all(len(item) == len(all_mags) for _, item in items)\n",
    "    assert all(set(item.mag for item in item) == set(all_mags) for _, item in items)\n",
    "    \n",
    "    # Initialize tensor with NaNs\n",
    "    tensor = np.full((len(datasets), len(layers), len(all_mags)), np.nan)\n",
    "    \n",
    "    # Fill tensor with mean lengths\n",
    "    for (dataset, layer), histos in items:\n",
    "        d_idx = dataset_to_idx[dataset]\n",
    "        l_idx = layer_to_idx[layer]\n",
    "        for histo in histos:\n",
    "            m_idx = mag_to_idx[histo.mag]\n",
    "            tensor[d_idx, l_idx, m_idx] = histo.mean\n",
    "    \n",
    "    assert all_mags.count(0) == 1\n",
    "    zero_mag_idx = all_mags.index(0)\n",
    "    return tensor, datasets, layers, all_mags, zero_mag_idx\n",
    "\n",
    "def normalize_tensor(tensor: np.ndarray, zero_mag_idx: int) -> np.ndarray:\n",
    "    \"\"\"Normalize tensor by zero-magnitude values\"\"\"\n",
    "    # Normalize each dataset and layer by its zero-magnitude value\n",
    "    normalized = np.zeros_like(tensor)\n",
    "    for d in range(tensor.shape[0]):\n",
    "        for l in range(tensor.shape[1]):\n",
    "            zero_val = tensor[d, l, zero_mag_idx]\n",
    "            if not np.isnan(zero_val) and zero_val != 0:\n",
    "                normalized[d, l, :] = tensor[d, l, :] / zero_val# * 100\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def visualize_tables(normalized_tensor, datasets, layers, magnitudes):\n",
    "    \"\"\"Create 6 visualizations (one per dataset)\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12)) # turns out we only had 4 datasets since we didn't use the testset lol\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for d_idx, dataset in enumerate(datasets):\n",
    "        ax = axes[d_idx]\n",
    "        data = normalized_tensor[d_idx]\n",
    "        \n",
    "        # Create a DataFrame for better visualization\n",
    "        df = pd.DataFrame(data, index=layers, columns=magnitudes)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", ax=ax, \n",
    "                    vmin=normalized_tensor.min().item(), vmax=normalized_tensor.max().item(), cbar_kws={'label': '% of zero-magnitude'})\n",
    "        \n",
    "        ax.set_title(f\"Dataset: {dataset}\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "        ax.set_xlabel(\"Magnitude\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"magnitude_impact_tables.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "# 1. Extract data into tensor\n",
    "data_tensor, datasets, layers, magnitudes, zero_mag_idx = extract_data_tensor(items)\n",
    "assert data_tensor.shape == (len(datasets), len(layers), len(magnitudes)), \"Tensor shape mismatch\"\n",
    "\n",
    "# 2. Normalize by zero-magnitude values\n",
    "normalized_tensor = normalize_tensor(data_tensor, zero_mag_idx)\n",
    "assert normalized_tensor.shape == data_tensor.shape, \"Normalized tensor shape mismatch\"\n",
    "\n",
    "# 3. Visualize the tables\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "visualize_tables(normalized_tensor, datasets, layers, magnitudes)\n",
    "\n",
    "print(f\"Tables visualization saved to {output_dir / 'magnitude_impact_tables.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifyoudont",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
